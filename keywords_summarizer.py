import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import time
import os
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

# 1. è‡ªåŠ¨è·å– Gemini å¯†é’¥ï¼ˆä» GitHub Secrets è¯»å–ï¼‰
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')

def fetch_abstract(arxiv_url):
    response = requests.get(arxiv_url)
    if response.status_code != 200:
        return f"Error: Unable to fetch {arxiv_url}, status code: {response.status_code}"
    soup = BeautifulSoup(response.text, 'html.parser')
    abstract_tag = soup.find('blockquote', class_='abstract mathjax')
    if abstract_tag:
        abstract_text = abstract_tag.text.strip()
        if abstract_text.startswith("Abstract:"):
            abstract_text = abstract_text.replace("Abstract:", "Abstract: ")
        return abstract_text
    else:
        return "Error: Abstract not found."

def summarize_with_gemini(abstract_text):
    url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=" + (GEMINI_API_KEY or "")
    headers = {"Content-Type": "application/json"}
    data = {
        "contents": [{
            "parts": [{
                "text": f"è¯·ç”¨ä¸­æ–‡ç®€è¦æ€»ç»“ä»¥ä¸‹è®ºæ–‡æ‘˜è¦ï¼ˆ1-2å¥è¯ï¼‰ã€‚é‡ç‚¹è¯´æ˜ä½œè€…åšäº†ä»€ä¹ˆã€ä¸ºä»€ä¹ˆåšä»¥åŠä¸»è¦ç»“æœ: \n\n{abstract_text}"
            }]
        }]
    }
    try:
        response = requests.post(url, headers=headers, json=data)
        if response.status_code == 200:
            result = response.json()
            summary = result['candidates'][0]['content']['parts'][0]['text']
            return summary
        else:
            return f"Error: API è¿”å›é”™è¯¯ä»£ç  {response.status_code}"
    except Exception as e:
        return f"Error: {str(e)}"

def fetch_papers_for_date_range(keyword, start_date, end_date, max_results):
    papers = []
    query = f'all:"{keyword}"'
    query_url = f"http://export.arxiv.org/api/query?search_query=({query})+AND+submittedDate:[{start_date}+TO+{end_date}]&start=0&max_results={max_results}"
    response = requests.get(query_url)
    if response.status_code != 200:
        return papers
    root = ET.fromstring(response.content)
    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
        title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip()
        summary = entry.find('{http://www.w3.org/2005/Atom}summary').text.strip()
        link_tag = entry.find('{http://www.w3.org/2005/Atom}link[@title="pdf"]')
        link = link_tag.attrib['href'] if link_tag is not None else "No Link"
        papers.append({'title': title, 'summary': summary, 'link': link, 'keyword': keyword})
    return papers

def fetch_papers(keywords, start_date, end_date, max_results_per_keyword):
    papers = []
    start_dt = datetime.strptime(start_date, "%Y-%m-%d")
    end_dt = datetime.strptime(end_date, "%Y-%m-%d")
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(fetch_papers_for_date_range, kw, start_date, end_date, max_results_per_keyword) for kw in keywords]
        for future in as_completed(futures):
            papers.extend(future.result())
    return papers

# --- ä¸»ç¨‹åºæ‰§è¡Œéƒ¨åˆ† ---
if __name__ == "__main__":
    # 2. è®¾å®šä½ çš„å…³é”®è¯
    keywords = ["Two-stage robust optimization", "Industrial park power system", "UAV","GRN","swarm"]
    
    # 3. è‡ªåŠ¨è®¡ç®—æ—¥æœŸèŒƒå›´
    end_date_obj = datetime.now()
    start_date_obj = end_date_obj - timedelta(days=2)
    start_date = start_date_obj.strftime("%Y-%m-%d")
    end_date = end_date_obj.strftime("%Y-%m-%d")
    
    max_results_per_keyword = 5

    with open("result.txt", "w", encoding="utf-8") as result_file:
        print(f"å¼€å§‹ä»»åŠ¡ï¼šæŠ“å– {start_date} è‡³ {end_date} çš„è®ºæ–‡")
        papers = fetch_papers(keywords, start_date, end_date, max_results_per_keyword)
        
        if not papers:
            result_file.write("ä»Šæ—¥æ— ç›¸å…³å…³é”®è¯è®ºæ–‡æ›´æ–°ã€‚")
        
        for paper in papers:
            print(f"æ­£åœ¨åˆ†æ: {paper['title']}")
            abstract = paper['summary']
            summary = summarize_with_gemini(abstract)
            
            output = f"å…³é”®è¯: {paper['keyword']}\næ ‡é¢˜: {paper['title']}\né“¾æ¥: {paper['link']}\næ€»ç»“: {summary}\n\n"
            result_file.write(output)
            time.sleep(2) # é¿å…è¯·æ±‚è¿‡å¿«

# --- åœ¨æ–‡ä»¶æœ€æœ«å°¾æ·»åŠ ä»¥ä¸‹å†…å®¹ ---
    # è¯»å–æ¨é€é’¥åŒ™
    push_key = os.getenv('PUSH_KEY')
    if push_key:
        import requests
        # è¯»å–åˆšåˆšå†™å¥½çš„ç»“æœæ–‡ä»¶å†…å®¹
        with open("result.txt", "r", encoding="utf-8") as f:
           # --- ä»»åŠ¡ç»“æŸï¼Œå¼€å§‹å¤„ç†æ¨é€é€»è¾‘ ---
    push_key = os.getenv('PUSH_KEY')
    if push_key:
        import requests
        print("æ­£åœ¨å‡†å¤‡å‘é€å¾®ä¿¡é€šçŸ¥...")
        
        # 1. è¯»å–æŠ“å–åˆ°çš„ç»“æœ
        with open("result.txt", "r", encoding="utf-8") as f:
            content = f.read().strip()
        
        # 2. å¦‚æœæ–‡ä»¶ä¸ºç©ºæˆ–å†…å®¹å¤ªçŸ­ï¼Œè¯´æ˜æ²¡æŠ“åˆ°è®ºæ–‡
        if not content or len(content) < 5:
            title = f"ä»Šæ—¥è®ºæ–‡æé†’ï¼šæš‚æ— æ›´æ–° ({datetime.now().strftime('%m/%d')})"
            desp = "â˜• æŠ¥å‘Šè€æ¿ï¼šåœ¨ä½ å…³æ³¨çš„ç§‘ç ”é¢†åŸŸï¼ˆä¸¤é˜¶æ®µé²æ£’ä¼˜åŒ–/ç”µåŠ›ç³»ç»Ÿ/æ— äººæœºï¼‰ï¼Œè¿‘ä¸¤æ—¥ ArXiv æš‚æ— æ–°è®ºæ–‡å‘å¸ƒã€‚äº«å—æ²¡æœ‰æ–‡çŒ®å‹åŠ›çš„ä¸€å¤©å§ï¼"
        else:
            title = f"ä»Šæ—¥è®ºæ–‡é€Ÿé€’ - {datetime.now().strftime('%m/%d')}"
            desp = "ğŸ’¡ æŠ¥å‘Šè€æ¿ï¼šGemini å·²ä¸ºä½ è¯»å®Œä»¥ä¸‹æœ€æ–°æ–‡çŒ®ï¼š\n\n" + content.replace("\n", "\n\n")

        # 3. å‘é€è‡³ Serveré…±
        push_url = f"https://sctapi.ftqq.com/{push_key}.send"
        data = {
            "title": title,
            "desp": desp
        }
        
        try:
            res = requests.post(push_url, data=data)
            print(f"å¾®ä¿¡æ¥å£è¿”å›: {res.text}")
        except Exception as e:
            print(f"å‘é€å¤±è´¥: {e}")

print("å…¨éƒ¨æµç¨‹å·²æ‰§è¡Œå®Œæ¯•ã€‚")
