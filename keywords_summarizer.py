import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import time
import os
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

# 1. è·å–å¯†é’¥
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
PUSH_KEY = os.getenv('PUSH_KEY')

def fetch_abstract(arxiv_url):
    try:
        response = requests.get(arxiv_url, timeout=15)
        if response.status_code != 200: return "Error"
        soup = BeautifulSoup(response.text, 'html.parser')
        abstract_tag = soup.find('blockquote', class_='abstract mathjax')
        return abstract_tag.text.strip() if abstract_tag else "Error"
    except:
        return "Error"

def summarize_with_gemini(abstract_text):
    if not GEMINI_API_KEY: return "æœªé…ç½® Gemini å¯†é’¥"
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}"
    headers = {"Content-Type": "application/json"}
    data = {
        "contents": [{
            "parts": [{"text": f"è¯·ç”¨ä¸­æ–‡ç®€è¦æ€»ç»“ä»¥ä¸‹æ‘˜è¦ï¼ˆ1-2å¥ï¼‰ï¼š\n\n{abstract_text}"}]
        }]
    }
    try:
        response = requests.post(url, headers=headers, json=data, timeout=30)
        result = response.json()
        return result['candidates'][0]['content']['parts'][0]['text']
    except:
        return "æ€»ç»“å¤±è´¥"

def fetch_papers_for_date_range(keyword, start_date, end_date, max_results):
    papers = []
    query = f'all:"{keyword}"'
    query_url = f"http://export.arxiv.org/api/query?search_query=({query})+AND+submittedDate:[{start_date}+TO+20261231235959]&start=0&max_results={max_results}"
    try:
        response = requests.get(query_url, timeout=15)
        root = ET.fromstring(response.content)
        for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):
            title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip()
            summary = entry.find('{http://www.w3.org/2005/Atom}summary').text.strip()
            link_tag = entry.find('{http://www.w3.org/2005/Atom}link[@title="pdf"]')
            link = link_tag.attrib['href'] if link_tag is not None else ""
            papers.append({'title': title, 'summary': summary, 'link': link, 'keyword': keyword})
    except:
        pass
    return papers

if __name__ == "__main__":
    # é…ç½®ä¿¡æ¯
    keywords = ["Two-stage robust optimization", "Industrial park power system", "UAV obstacle avoidance"]
    end_date = datetime.now().strftime("%Y%m%d%H%M%S")
    start_date = (datetime.now() - timedelta(days=2)).strftime("%Y%m%d%H%M%S")
    
    final_content = ""
    
    print(f"å¼€å§‹ä»»åŠ¡...")
    for kw in keywords:
        found_papers = fetch_papers_for_date_range(kw, start_date, end_date, 3)
        for paper in found_papers:
            summary = summarize_with_gemini(paper['summary'])
            item = f"ã€{paper['keyword']}ã€‘\næ ‡é¢˜ï¼š{paper['title']}\næ€»ç»“ï¼š{summary}\né“¾æ¥ï¼š{paper['link']}\n\n"
            final_content += item
            time.sleep(2)

    # å†™å…¥ç»“æœæ–‡ä»¶
    with open("result.txt", "w", encoding="utf-8") as f:
        f.write(final_content if final_content else "ä»Šæ—¥æ— æ›´æ–°")

    # æ¨é€é€»è¾‘
    if PUSH_KEY:
        title = f"æ¯æ—¥æ–‡çŒ®é€Ÿé€’-{datetime.now().strftime('%m/%d')}"
        if not final_content:
            desp = "â˜• æŠ¥å‘Šè€æ¿ï¼šè¿‘ä¸¤æ—¥ä½ å…³æ³¨çš„é¢†åŸŸæš‚æ— æ–°è®ºæ–‡å‘å¸ƒã€‚"
        else:
            desp = "ğŸ’¡ æŠ¥å‘Šè€æ¿ï¼šä»Šæ—¥æœ€æ–°è®ºæ–‡æ€»ç»“å¦‚ä¸‹ï¼š\n\n" + final_content.replace("\n", "\n\n")
        
        requests.post(f"https://sctapi.ftqq.com/{PUSH_KEY}.send", data={"title": title, "desp": desp})

    print("å…¨éƒ¨æµç¨‹æ‰§è¡Œå®Œæ¯•")
